# Session 4: MCP, Plugins & Marketplaces
## Cut the Crap â€” AI Engineer Edition

---

## Slide 1: The Problem MCP Solves (Topic 16)

**SHOW:**
```
Before MCP:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    custom code    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claude   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  GitHub   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    custom code    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPT-5.2   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  GitHub   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    custom code    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemini   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  GitHub   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

N models Ã— M tools = NÃ—M integrations  ðŸ˜±

After MCP:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claude   â”‚â—„â”€â”€â”              â”‚  GitHub   â”‚â—„â”€â”€ MCP Server
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”œâ”€â”€â”€â–ºâ”‚ MCP  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPT-5.2   â”‚â—„â”€â”€â”¤   â”‚Protocolâ”‚â–ºâ”‚  Slack    â”‚â—„â”€â”€ MCP Server
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemini   â”‚â—„â”€â”€â”˜              â”‚  Database â”‚â—„â”€â”€ MCP Server
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

N + M integrations  âœ…
```

**SAY:**
> MCP â€” Model Context Protocol â€” is an open standard from Anthropic that's been adopted across the industry. The problem: if you have 3 AI models and 10 tools, you need 30 custom integrations. MCP creates a universal plug â€” any MCP server works with any MCP client. Build the GitHub integration once, it works everywhere. It's USB-C for AI tools.

---

## Slide 2: MCP Architecture

**SHOW:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MCP HOST                        â”‚
â”‚  (Claude Desktop, OpenClaw, VS Code, your app)  â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  MCP Client  â”‚  â”‚  MCP Client  â”‚  ...         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ stdio/SSE        â”‚ stdio/SSE
          â–¼                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  MCP Server  â”‚  â”‚  MCP Server  â”‚
   â”‚  (GitHub)    â”‚  â”‚  (Postgres)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MCP Server exposes:
  ðŸ“‹ Tools      â€” functions the AI can call
  ðŸ“„ Resources  â€” data the AI can read (files, DB records)
  ðŸ’¬ Prompts    â€” reusable prompt templates

Transport:
  stdio  â€” local process (most common)
  SSE    â€” remote over HTTP
```

**SAY:**
> Here's the architecture. The Host is your application â€” Claude Desktop, OpenClaw, VS Code with Copilot. Inside it, MCP Clients connect to MCP Servers. Each server exposes tools, resources, and prompts. Transport is usually stdio for local servers â€” the host spawns a process and communicates over stdin/stdout. For remote servers, it's SSE over HTTP. The key insight: the server is just a process that speaks a JSON-RPC protocol.

---

## Slide 3: What MCP Servers Exist (Topic 17)

**SHOW:**
```
Popular MCP Servers (as of 2026):

Filesystem & Code:
  ðŸ“ filesystem    â€” read/write/search files
  ðŸ”§ git           â€” clone, diff, commit, log
  ðŸ’» github        â€” issues, PRs, repos, actions

Data:
  ðŸ˜ postgres      â€” query PostgreSQL databases
  ðŸ“Š sqlite        â€” local SQLite databases
  ðŸ” elasticsearch â€” search and analytics

Communication:
  ðŸ’¬ slack          â€” channels, messages, users
  ðŸ“§ gmail         â€” read/send email
  ðŸ“ notion        â€” pages, databases

Web:
  ðŸŒ brave-search  â€” web search
  ðŸ•·ï¸ puppeteer     â€” browser automation
  ðŸ“¡ fetch         â€” HTTP requests

Dev Tools:
  ðŸ³ docker        â€” container management
  â˜ï¸  aws           â€” AWS service access
  ðŸ“¦ npm           â€” package info
```

**SAY:**
> The ecosystem is huge. Filesystem gives AI read/write access to your files. GitHub lets it manage issues and PRs. Postgres lets it query your database directly. Slack lets it read and send messages. These aren't toy demos â€” production teams use these daily. The community maintains hundreds of servers. If one doesn't exist for your tool, building one is straightforward.

---

## Slide 4: Setting Up MCP â€” Claude Desktop (Topic 18)

**SHOW:**
```json
// ~/Library/Application Support/Claude/claude_desktop_config.json
// (macOS) or %APPDATA%/Claude/claude_desktop_config.json (Windows)

{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": [
        "-y",
        "@modelcontextprotocol/server-filesystem",
        "/Users/you/projects"
      ]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_..."
      }
    },
    "postgres": {
      "command": "npx",
      "args": [
        "-y",
        "@modelcontextprotocol/server-postgres",
        "postgresql://user:pass@localhost:5432/mydb"
      ]
    }
  }
}
```

```
Setup steps:
1. Install Node.js (npx comes with it)
2. Edit the config file above
3. Restart Claude Desktop
4. Look for ðŸ”Œ icon â€” tools appear automatically
5. Ask Claude: "List the files in my projects folder"
```

**SAY:**
> Live demo time. This is the Claude Desktop config file. You add MCP servers here and restart. Each server has a command to run and optional environment variables. The filesystem server needs a path to expose. GitHub needs a token. Postgres needs a connection string. After restart, Claude Desktop shows a plug icon â€” click it to see available tools. Then just ask Claude to do things and it calls the tools automatically. Let's set this up right now.

---

## Slide 5: Setting Up MCP â€” VS Code / Cursor

**SHOW:**
```json
// .vscode/mcp.json (project-level)
{
  "servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "${workspaceFolder}"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "${env:GITHUB_TOKEN}"
      }
    }
  }
}
```

```
// For Cursor: similar config in Cursor settings
// For Claude Code CLI:
// ~/.claude/settings.json or project-level .mcp.json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "."]
    }
  }
}
```

**SAY:**
> VS Code and Cursor also support MCP. Put a `mcp.json` in your `.vscode` folder and the AI assistant gets tool access. Cursor has similar support. Claude Code CLI reads from `.mcp.json` in your project. The config format is nearly identical everywhere â€” that's the point of a standard protocol.

---

## Slide 6: Building a Simple MCP Server

**SHOW:**
```python
# my_mcp_server.py â€” A minimal MCP server in Python
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import Tool, TextContent

server = Server("my-tools")

@server.list_tools()
async def list_tools():
    return [
        Tool(
            name="word_count",
            description="Count words in a text",
            inputSchema={
                "type": "object",
                "properties": {
                    "text": {"type": "string", "description": "Text to count words in"}
                },
                "required": ["text"]
            }
        ),
        Tool(
            name="reverse_string",
            description="Reverse a string",
            inputSchema={
                "type": "object",
                "properties": {
                    "text": {"type": "string"}
                },
                "required": ["text"]
            }
        ),
    ]

@server.call_tool()
async def call_tool(name: str, arguments: dict):
    if name == "word_count":
        count = len(arguments["text"].split())
        return [TextContent(type="text", text=f"Word count: {count}")]
    elif name == "reverse_string":
        return [TextContent(type="text", text=arguments["text"][::-1])]
    else:
        raise ValueError(f"Unknown tool: {name}")

async def main():
    async with stdio_server() as (read, write):
        await server.run(read, write, server.create_initialization_options())

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

```bash
# Install the MCP Python SDK
pip install mcp

# Register in Claude Desktop config:
# "my-tools": {"command": "python", "args": ["my_mcp_server.py"]}
```

**SAY:**
> Building an MCP server is surprisingly simple. The Python SDK gives you a Server class. You decorate functions to list tools and handle calls. The server communicates over stdio â€” Claude Desktop spawns it as a process. This is maybe 40 lines of real code. You could wrap any internal API, any database, any service as an MCP server in under an hour.

---

## Slide 7: MCP Resources & Prompts

**SHOW:**
```python
from mcp.types import Resource, Prompt, PromptMessage, PromptArgument

# Resources â€” data the AI can read
@server.list_resources()
async def list_resources():
    return [
        Resource(
            uri="config://app-settings",
            name="App Settings",
            description="Current application configuration",
            mimeType="application/json",
        )
    ]

@server.read_resource()
async def read_resource(uri: str):
    if uri == "config://app-settings":
        return json.dumps({"debug": True, "version": "2.1.0"})

# Prompts â€” reusable prompt templates
@server.list_prompts()
async def list_prompts():
    return [
        Prompt(
            name="code-review",
            description="Review code for bugs and improvements",
            arguments=[
                PromptArgument(name="language", description="Programming language"),
                PromptArgument(name="code", description="Code to review"),
            ]
        )
    ]

@server.get_prompt()
async def get_prompt(name: str, arguments: dict):
    if name == "code-review":
        return {
            "messages": [
                PromptMessage(role="user", content=TextContent(
                    type="text",
                    text=f"Review this {arguments['language']} code for bugs, "
                         f"security issues, and improvements:\n\n{arguments['code']}"
                ))
            ]
        }
```

**SAY:**
> MCP isn't just tools. Resources let the AI read data â€” config files, database records, API state. The AI can browse available resources and read what it needs. Prompts are reusable templates â€” think of them as saved prompt recipes the AI can use. In practice, tools are used 90% of the time, but resources and prompts round out the protocol.

---

## Slide 8: Marketplaces (Topic 19)

**SHOW:**
```
GPT Store (OpenAI):
  - Largest marketplace
  - Custom GPTs built by anyone
  - Revenue sharing for creators
  - Quality varies wildly
  - Accessible from ChatGPT

ClawHub (OpenClaw):
  - Skills marketplace for OpenClaw agents
  - Code-based â€” more powerful than GPTs
  - MCP server integration
  - Community-driven
  - Growing ecosystem

Community MCP Servers:
  - github.com/modelcontextprotocol/servers (official)
  - github.com/punkpeye/awesome-mcp-servers (community list)
  - npm/PyPI packages â€” install and configure
  - No centralized "store" yet â€” it's early

Smithery.ai:
  - MCP server directory and registry
  - One-click install for supported hosts
  - Growing catalog
```

**SAY:**
> Marketplaces are still early. The GPT Store is the biggest but quality is all over the place. ClawHub is where OpenClaw skills live â€” we'll build one in Session 8. For MCP, there's no single store yet â€” you find servers on GitHub, npm, and directories like Smithery. The official MCP GitHub has reference servers. The awesome-mcp-servers list is community-curated. This space will consolidate over the next year.

---

## Slide 9: Hands-On â€” Connect MCP (Topic 20)

**SHOW:**
```
ðŸ“ Exercise: Set up MCP in Claude Desktop (or OpenClaw)

Part 1 â€” Connect existing servers (10 min):
  1. Add filesystem MCP server to Claude Desktop
  2. Add GitHub MCP server (create a token first)
  3. Ask Claude to list files, read a file, create a file
  4. Ask Claude to list your GitHub repos

Part 2 â€” Build your own server (20 min):
  1. Create a simple MCP server with 2-3 custom tools
     Ideas: todo list, dictionary lookup, unit converter
  2. Register it in Claude Desktop config
  3. Test it by chatting with Claude

Bonus:
  - Set up the Postgres MCP server with a local database
  - Ask Claude to write and run SQL queries
```

**SAY:**
> Two-part exercise. First, connect the official filesystem and GitHub MCP servers to Claude Desktop. This should take 10 minutes â€” it's just config. Then build your own MCP server with custom tools. Use the template from slide 6. Register it, restart Claude Desktop, and test it. If you finish early, try the Postgres server â€” there's nothing quite like asking Claude to query your database in natural language.

---

## Slide 10: MCP Security Considerations

**SHOW:**
```
âš ï¸ MCP Security â€” Think Before You Connect

Filesystem Server:
  - Only expose directories you intend to
  - AI CAN write/delete files if the server allows it
  - Use read-only mode when possible

Database Servers:
  - Use read-only database users when possible
  - Never connect to production DBs without safeguards
  - Review queries before execution (some clients show them)

GitHub/Slack/Email:
  - Use tokens with minimum necessary permissions
  - The AI can send messages, create issues, merge PRs
  - Audit what actions the AI takes

General:
  âœ… Principle of least privilege
  âœ… Review tool calls before they execute
  âœ… Use sandboxed/dev environments first
  âœ… Log everything
  âŒ Don't give AI access to production systems without approval flows
```

**SAY:**
> Quick but critical: MCP gives AI real power. That filesystem server can delete files. That GitHub server can merge PRs. That database server can run DELETE queries. Principle of least privilege â€” expose only what's needed, use read-only tokens where possible, and always test in dev first. Most MCP clients show you what the AI wants to do before executing it. Pay attention to those prompts.

---

## Slide 11: Session 4 Recap

**SHOW:**
```
âœ… MCP = USB-C for AI tools â€” universal, open standard
âœ… Architecture: Host â†’ Client â†’ Server (stdio or SSE)
âœ… Servers expose: Tools, Resources, Prompts
âœ… Setup: JSON config in Claude Desktop / VS Code / OpenClaw
âœ… Building servers: ~40 lines with Python SDK
âœ… Ecosystem: 100s of servers on GitHub, Smithery, npm
âœ… Marketplaces: GPT Store, ClawHub, community MCP
âœ… Security: least privilege, audit, sandbox first

Sessions 1-4 complete! You now know:
  â†’ The landscape & APIs
  â†’ Prompt engineering & structured output
  â†’ Tool use & function calling
  â†’ MCP & the tool ecosystem

Next half: Agents, RAG, Evals, Production
```

**SAY:**
> That's the first half of the course done. You've gone from making your first API call to building tool-calling assistants to connecting universal tool protocols. Sessions 5-8 build on everything: agents that chain multiple tools together, RAG for working with your own data, evals and observability for production quality, and finally shipping real AI applications. See you in Session 5.
